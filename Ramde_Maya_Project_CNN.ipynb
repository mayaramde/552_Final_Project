{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KF8J-OLcd1cg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path as op\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import logging\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from skimage import io\n",
        "import keras\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "random_state = 42\n",
        "from PIL import Image\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras import layers, models, regularizers, callbacks, preprocessing, utils, applications, optimizers\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, MaxPooling2D, Dropout, Flatten, Dense, GlobalMaxPooling2D, Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.applications import efficientnet, vgg16, EfficientNetB0\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install tensorflow"
      ],
      "metadata": {
        "id": "W44XINje6W30"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive to Colab notebook\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF16z2MwgT85",
        "outputId": "1dba85b5-9d4a-4e01-d8e1-72860882ae0b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploaded data.zip to my google drive. Now, unzip it and store in 552_project_data folder\n",
        "# zip_file_path = '/content/drive/MyDrive/Colab Notebooks/data.zip'\n",
        "# destination_folder = '/content/drive/MyDrive/552_project_data/'\n",
        "# !unzip \"$zip_file_path\" -d \"$destination_folder\""
      ],
      "metadata": {
        "id": "KQ6y6B2nh3ix"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QGkoMFtod1ch"
      },
      "outputs": [],
      "source": [
        "# Logging configuration\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    datefmt='%H:%M:%S',\n",
        "                    format='%(asctime)s | %(levelname)-5s | %(module)-15s | %(message)s')\n",
        "\n",
        "IMAGE_SIZE = (100, 100)  # All images contained in this dataset are 299x299 (originally, to match Inception v3 input size)\n",
        "SEED = 17\n",
        "\n",
        "# Head directory containing all image subframes. Update with the relative path of your data directory\n",
        "data_head_dir = Path('/content/drive/MyDrive/552_project_data/data')\n",
        "\n",
        "# Find all subframe directories\n",
        "subdirs = [Path(subdir.stem) for subdir in data_head_dir.iterdir() if subdir.is_dir()]\n",
        "src_image_ids = ['_'.join(a_path.name.split('_')[:3]) for a_path in subdirs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "XPU_EQM8d1ci"
      },
      "outputs": [],
      "source": [
        "# Load train/val/test subframe IDs\n",
        "def load_text_ids(file_path):\n",
        "    \"\"\"Simple helper to load all lines from a text file\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = [line.strip() for line in f.readlines()]\n",
        "    return lines\n",
        "\n",
        "# Load the subframe names for the three data subsets\n",
        "train_ids = load_text_ids('/content/drive/MyDrive/Colab Notebooks/train_source_images.txt')\n",
        "validate_ids = load_text_ids('/content/drive/MyDrive/Colab Notebooks/val_source_images.txt')\n",
        "test_ids = load_text_ids('/content/drive/MyDrive/Colab Notebooks/test_source_images.txt')\n",
        "\n",
        "# Generate a list containing the dataset split for the matching subdirectory names\n",
        "subdir_splits = []\n",
        "for src_id in src_image_ids:\n",
        "    if src_id in train_ids:\n",
        "        subdir_splits.append('train')\n",
        "    elif src_id in validate_ids:\n",
        "        subdir_splits.append('validate')\n",
        "    elif(src_id in test_ids):\n",
        "        subdir_splits.append('test')\n",
        "    else:\n",
        "        logging.warning(f'{src_id}: Did not find designated split in train/validate/test list.')\n",
        "        subdir_splits.append(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uoXB_Xcd1ci"
      },
      "source": [
        "### Loading and pre processing the data for (224, 244) image size\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Must resize the images and recreate train/test/validation data sets\n",
        "def load_and_preprocess_v2(img_loc, label):\n",
        "    def _inner_function_v2(img_loc, label):\n",
        "      img_loc_str = img_loc.numpy().decode('utf-8')\n",
        "      img = Image.open(img_loc_str).convert('RGB')\n",
        "      img = img.resize((100, 100), Image.ANTIALIAS) # resize\n",
        "      img = np.array(img)\n",
        "      img = img / 255.0\n",
        "      label = 1 if label.numpy().decode('utf-8') == 'frost' else 0\n",
        "      return img, label\n",
        "\n",
        "    # Wrap the Python function\n",
        "    X, y = tf.py_function(_inner_function_v2, [img_loc, label], [tf.float32, tf.int64])\n",
        "\n",
        "    # Set the shape of the tensors\n",
        "    X.set_shape([100, 100, 3])  # new image size\n",
        "    y.set_shape([])  # Scalar label\n",
        "    return X, y\n",
        "\n",
        "def load_subdir_data(dir_path, image_size, seed=None):\n",
        "    # Grab only the classes that (1) we want to keep and (2) exist in this directory\n",
        "    tile_dir = dir_path / Path('tiles')\n",
        "    label_dir = dir_path /Path('labels')\n",
        "\n",
        "    loc_list = []\n",
        "\n",
        "    for folder in os.listdir(tile_dir):\n",
        "        if os.path.isdir(os.path.join(tile_dir, folder)):\n",
        "            for file in os.listdir(os.path.join(tile_dir, folder)):\n",
        "                if file.endswith(\".png\"):\n",
        "                    loc_list.append((os.path.join(os.path.join(tile_dir, folder), file), folder))\n",
        "    return loc_list\n",
        "\n",
        "\n",
        "\n",
        "IMAGE_SIZE = (100, 100)\n",
        "\n",
        "# Loop over all subframes, loading each into a list\n",
        "tf_data_train, tf_data_test, tf_data_val = [], [], []\n",
        "tf_dataset_train_v2, tf_dataset_test_v2, tf_dataset_val_v2 = [], [], []\n",
        "\n",
        "# Update the batch and buffer size as per your model requirements\n",
        "buffer_size = 64\n",
        "batch_size = 5\n",
        "\n",
        "for subdir, split in zip(subdirs, subdir_splits):\n",
        "    full_path = data_head_dir / subdir\n",
        "    if split=='validate':\n",
        "        tf_data_val.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
        "    elif split=='train':\n",
        "        tf_data_train.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
        "    elif split=='test':\n",
        "        tf_data_test.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
        "\n",
        "random.shuffle(tf_data_train)\n",
        "img_list, label_list = zip(*tf_data_train)\n",
        "img_list_t = tf.convert_to_tensor(img_list)\n",
        "lb_list_t = tf.convert_to_tensor(label_list)\n",
        "\n",
        "tf_dataset_train_v2= tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
        "tf_dataset_train_v2= tf_dataset_train_v2.map(load_and_preprocess_v2, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "tf_dataset_train_v2 = tf_dataset_train_v2.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
        "\n",
        "random.shuffle(tf_data_val)\n",
        "img_list, label_list = zip(*tf_data_val)\n",
        "img_list_t = tf.convert_to_tensor(img_list)\n",
        "lb_list_t = tf.convert_to_tensor(label_list)\n",
        "\n",
        "tf_dataset_val_v2 = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
        "tf_dataset_val_v2 = tf_dataset_val_v2.map(load_and_preprocess_v2, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "tf_dataset_val_v2 = tf_dataset_val_v2.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
        "\n",
        "random.shuffle(tf_data_test)\n",
        "img_list, label_list = zip(*tf_data_test)\n",
        "img_list_t = tf.convert_to_tensor(img_list)\n",
        "lb_list_t = tf.convert_to_tensor(label_list)\n",
        "\n",
        "tf_dataset_test_v2 = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
        "tf_dataset_test_v2 = tf_dataset_test_v2.map(load_and_preprocess_v2, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "tf_dataset_test_v2 = tf_dataset_test_v2.shuffle(buffer_size=buffer_size).batch(batch_size)"
      ],
      "metadata": {
        "id": "ac5a_M84Hd2k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Dataset Length:\", len(tf_dataset_train_v2))\n",
        "print(\"Validation Dataset Length:\", len(tf_dataset_val_v2))\n",
        "print(\"Testing Dataset Length:\", len(tf_dataset_test_v2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pDi1UKqmP1Z",
        "outputId": "c034d310-9dd1-41e2-f58f-61eaf1276d81"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Length: 2824\n",
            "Validation Dataset Length: 1578\n",
            "Testing Dataset Length: 1595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7Ekkpaj0Z-N",
        "outputId": "2fb0027f-0c0a-470e-9f09-4c5ce909a0ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/611.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m604.2/611.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment(image, label):\n",
        "    # apply brightness and contrast adjustments\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "\n",
        "    # apply rotation\n",
        "    angle = tf.random.uniform([], minval=-math.pi/4, maxval=math.pi/4)\n",
        "    image = tfa.image.rotate(image, angle)\n",
        "\n",
        "    # flip left-right\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "    # flip up-down\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "\n",
        "    # adjust hue and saturation\n",
        "    image = tf.image.random_hue(image, max_delta=0.2)\n",
        "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "# Apply the augment function to each element in the dataset\n",
        "tf_dataset_train_v2 = tf_dataset_train_v2.map(augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "LCIZJH9uDHFh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## d) CNN Model"
      ],
      "metadata": {
        "id": "PMFPxk3L9FW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Train a three-layer CNN followed by a dense layer on the data. Choose the size of the kernels and depth of the layers and the number of neurons in the dense layer (MLP) on your own. Use ReLU’s in all of the layers. Use the softmax function, batch normalization3 and a dropout rate of 30%, L2 regularization, as well as ADAM optimizer. Use cross entropy loss. Train for at least 20 epochs and perform early stopping using the validation set. Keep the network parameters that have the lowest validation error. Plot the training and validation errors vs. epochs."
      ],
      "metadata": {
        "id": "SlkvY0rr9JCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_cnn = Sequential([\n",
        "    # block 1\n",
        "    Conv2D(32, (3, 3), padding = 'same', input_shape = (100, 100, 3), kernel_regularizer = l2(0.001)),\n",
        "    BatchNormalization(), ReLU(), MaxPooling2D(pool_size = (2, 2)),\n",
        "\n",
        "    # block 2\n",
        "    Conv2D(64, (3, 3), padding = 'same', kernel_regularizer = l2(0.001)),\n",
        "    BatchNormalization(), ReLU(), MaxPooling2D(pool_size = (2, 2)),\n",
        "\n",
        "    # block 3\n",
        "    Conv2D(128, (3, 3), padding = 'same', kernel_regularizer = l2(0.001)),\n",
        "    BatchNormalization(), ReLU(), MaxPooling2D(pool_size = (2, 2)),\n",
        "\n",
        "    # classifier\n",
        "    Flatten(),\n",
        "    Dense(128, activation = 'relu', kernel_regularizer = l2(0.001)),\n",
        "    Dropout(0.3), # 30% dropout rate\n",
        "    Dense(2, activation = 'softmax')\n",
        "])\n",
        "\n",
        "# Compile model with ADAM optimizer and cross entropy loss\n",
        "model_cnn.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping with patience = 3 and model checkpointing\n",
        "early_stop_criteria = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_model_res.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "history_cnn = model_cnn.fit(\n",
        "    tf_dataset_train_v2,\n",
        "    epochs=20,\n",
        "    validation_data=tf_dataset_val_v2,\n",
        "    callbacks=[early_stop_criteria, checkpoint]\n",
        ")\n",
        "\n",
        "# Save model at each epoch\n",
        "for epoch in range(1, 20):\n",
        "    model_cnn.save(f'cnn_model_at_epoch_{epoch}.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTp9PIftkm2o",
        "outputId": "b3dab437-a51d-4a20-d584-07e28c95109f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "2824/2824 [==============================] - 2717s 961ms/step - loss: 0.7899 - accuracy: 0.7036 - val_loss: 0.8301 - val_accuracy: 0.7565\n",
            "Epoch 2/20\n",
            "2824/2824 [==============================] - 728s 258ms/step - loss: 0.5715 - accuracy: 0.7104 - val_loss: 0.6479 - val_accuracy: 0.8589\n",
            "Epoch 3/20\n",
            "2824/2824 [==============================] - 740s 262ms/step - loss: 0.5530 - accuracy: 0.7075 - val_loss: 0.6092 - val_accuracy: 0.8716\n",
            "Epoch 4/20\n",
            "2824/2824 [==============================] - 746s 264ms/step - loss: 0.5395 - accuracy: 0.7130 - val_loss: 0.6346 - val_accuracy: 0.8365\n",
            "Epoch 5/20\n",
            "2824/2824 [==============================] - 792s 280ms/step - loss: 0.5387 - accuracy: 0.7154 - val_loss: 0.7994 - val_accuracy: 0.7750\n",
            "Epoch 6/20\n",
            "2824/2824 [==============================] - 734s 260ms/step - loss: 0.5209 - accuracy: 0.7238 - val_loss: 0.5674 - val_accuracy: 0.8843\n",
            "Epoch 7/20\n",
            "2824/2824 [==============================] - 721s 255ms/step - loss: 0.5128 - accuracy: 0.7265 - val_loss: 0.5594 - val_accuracy: 0.8788\n",
            "Epoch 8/20\n",
            "2824/2824 [==============================] - 700s 248ms/step - loss: 0.5053 - accuracy: 0.7312 - val_loss: 0.5905 - val_accuracy: 0.8509\n",
            "Epoch 9/20\n",
            "2824/2824 [==============================] - 650s 230ms/step - loss: 0.5020 - accuracy: 0.7297 - val_loss: 0.6219 - val_accuracy: 0.8545\n",
            "Epoch 10/20\n",
            "2824/2824 [==============================] - 714s 253ms/step - loss: 0.4946 - accuracy: 0.7312 - val_loss: 0.5405 - val_accuracy: 0.8777\n",
            "Epoch 11/20\n",
            "2824/2824 [==============================] - 706s 250ms/step - loss: 0.5000 - accuracy: 0.7276 - val_loss: 0.5510 - val_accuracy: 0.8744\n",
            "Epoch 12/20\n",
            "2824/2824 [==============================] - 704s 249ms/step - loss: 0.4851 - accuracy: 0.7384 - val_loss: 0.7444 - val_accuracy: 0.7813\n",
            "Epoch 13/20\n",
            "2824/2824 [==============================] - 700s 248ms/step - loss: 0.4887 - accuracy: 0.7384 - val_loss: 0.5395 - val_accuracy: 0.8816\n",
            "Epoch 14/20\n",
            "2824/2824 [==============================] - 655s 232ms/step - loss: 0.4854 - accuracy: 0.7389 - val_loss: 0.6005 - val_accuracy: 0.8514\n",
            "Epoch 15/20\n",
            " 255/2824 [=>............................] - ETA: 8:42 - loss: 0.4844 - accuracy: 0.7380"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the training and validation errors vs. epochs."
      ],
      "metadata": {
        "id": "cnHfNMnyqwd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Error rates\n",
        "cnn_train_error, cnn_validation_error = [], []\n",
        "\n",
        "train_accuracy = history_cnn.history['accuracy']\n",
        "val_accuracy = history_cnn.history['val_accuracy']\n",
        "\n",
        "for i in range(len(train_accuracy)):\n",
        "    cnn_train_error.append(1 - train_accuracy[i])\n",
        "    cnn_validation_error.append(1 - val_accuracy[i])\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Training & Validation Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_cnn.history['loss'], label = 'Training Loss', marker = 'o')\n",
        "plt.plot(history_cnn.history['val_loss'], label = 'Validation Loss', marker = 'o')\n",
        "plt.title('CNN Model: Training & Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot for Training and Validation Error\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(cnn_train_error, label = 'Training Error', marker = 'o')\n",
        "plt.plot(cnn_validation_error, label = 'Validation Error', marker = 'o')\n",
        "plt.title('CNN Model: Training & Validation Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Error Rate')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ri1d3ecgZ8DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Report Precision, Recall, and F1 score for CNN model."
      ],
      "metadata": {
        "id": "7YPsq-MUaKoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best model = least validation loss\n",
        "best_cnn_model = load_model('best_model_res.h5')\n",
        "\n",
        "# run model with test data\n",
        "effnet_predictions = best_cnn_model.predict(tf_dataset_test_v2)\n",
        "predicted_labels = np.argmax(effnet_predictions, axis=1)\n",
        "\n",
        "# pull actual Labels from test data\n",
        "actual_labels = []\n",
        "for images, labels in tf_dataset_test_v2.unbatch():\n",
        "    actual_labels.append(labels.numpy())\n",
        "\n",
        "cnn_report = classification_report(np.array(actual_labels).flatten(), predicted_labels.flatten())\n",
        "print(\"CNN Model Classification Report:\")\n",
        "print(cnn_report)"
      ],
      "metadata": {
        "id": "vCszBZUsaK67"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}