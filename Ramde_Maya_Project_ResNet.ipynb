{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KF8J-OLcd1cg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path as op\n",
        "import json\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import logging\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from skimage import io\n",
        "import keras\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "from PIL import Image\n",
        "from tensorflow.keras import layers, models, regularizers, callbacks\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, MaxPooling2D, Dropout, Flatten, Dense\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.applications import efficientnet, vgg16\n",
        "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout, GlobalMaxPooling2D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "random_state = 42"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "W44XINje6W30",
        "outputId": "ad7f6925-ad73-410b-a6e6-cc20da9417b2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.3)\n",
            "Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: keras<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.15,>=2.14->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive to Colab notebook\n",
        "from google.colab import drive\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF16z2MwgT85",
        "outputId": "da81852c-dfe0-48c4-a067-9c05eedfb752"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploaded data.zip to my google drive. Now, unzip it and store in 552_project_data folder\n",
        "# zip_file_path = '/content/drive/MyDrive/Colab Notebooks/data.zip'\n",
        "# destination_folder = '/content/drive/MyDrive/552_project_data/'\n",
        "# !unzip \"$zip_file_path\" -d \"$destination_folder\""
      ],
      "metadata": {
        "id": "KQ6y6B2nh3ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QGkoMFtod1ch"
      },
      "outputs": [],
      "source": [
        "# Logging configuration\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    datefmt='%H:%M:%S',\n",
        "                    format='%(asctime)s | %(levelname)-5s | %(module)-15s | %(message)s')\n",
        "\n",
        "IMAGE_SIZE = (299, 299)  # All images contained in this dataset are 299x299 (originally, to match Inception v3 input size)\n",
        "SEED = 17\n",
        "\n",
        "# Head directory containing all image subframes. Update with the relative path of your data directory\n",
        "data_head_dir = Path('/content/drive/MyDrive/552_project_data/data')\n",
        "\n",
        "# Find all subframe directories\n",
        "subdirs = [Path(subdir.stem) for subdir in data_head_dir.iterdir() if subdir.is_dir()]\n",
        "src_image_ids = ['_'.join(a_path.name.split('_')[:3]) for a_path in subdirs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XPU_EQM8d1ci"
      },
      "outputs": [],
      "source": [
        "# Load train/val/test subframe IDs\n",
        "def load_text_ids(file_path):\n",
        "    \"\"\"Simple helper to load all lines from a text file\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = [line.strip() for line in f.readlines()]\n",
        "    return lines\n",
        "\n",
        "# Load the subframe names for the three data subsets\n",
        "train_ids = load_text_ids('/content/drive/MyDrive/Colab Notebooks/train_source_images.txt')\n",
        "validate_ids = load_text_ids('/content/drive/MyDrive/Colab Notebooks/val_source_images.txt')\n",
        "test_ids = load_text_ids('/content/drive/MyDrive/Colab Notebooks/test_source_images.txt')\n",
        "\n",
        "# Generate a list containing the dataset split for the matching subdirectory names\n",
        "subdir_splits = []\n",
        "for src_id in src_image_ids:\n",
        "    if src_id in train_ids:\n",
        "        subdir_splits.append('train')\n",
        "    elif src_id in validate_ids:\n",
        "        subdir_splits.append('validate')\n",
        "    elif(src_id in test_ids):\n",
        "        subdir_splits.append('test')\n",
        "    else:\n",
        "        logging.warning(f'{src_id}: Did not find designated split in train/validate/test list.')\n",
        "        subdir_splits.append(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uoXB_Xcd1ci"
      },
      "source": [
        "### Loading and pre processing the data for (224, 244) image size\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Must resize the images and recreate train/test/validation data sets\n",
        "def load_and_preprocess_v2(img_loc, label):\n",
        "    def _inner_function_v2(img_loc, label):\n",
        "      img_loc_str = img_loc.numpy().decode('utf-8')\n",
        "      img = Image.open(img_loc_str).convert('RGB')\n",
        "      img = img.resize((224, 224), Image.ANTIALIAS) # resize\n",
        "      img = np.array(img)\n",
        "      img = img / 255.0\n",
        "      label = 1 if label.numpy().decode('utf-8') == 'frost' else 0\n",
        "      return img, label\n",
        "\n",
        "    # Wrap the Python function\n",
        "    X, y = tf.py_function(_inner_function_v2, [img_loc, label], [tf.float32, tf.int64])\n",
        "\n",
        "    # Set the shape of the tensors\n",
        "    X.set_shape([224, 224, 3])  # new image size\n",
        "    y.set_shape([])  # Scalar label\n",
        "    return X, y\n",
        "\n",
        "def load_subdir_data(dir_path, image_size, seed=None):\n",
        "    # Grab only the classes that (1) we want to keep and (2) exist in this directory\n",
        "    tile_dir = dir_path / Path('tiles')\n",
        "    label_dir = dir_path /Path('labels')\n",
        "\n",
        "    loc_list = []\n",
        "\n",
        "    for folder in os.listdir(tile_dir):\n",
        "        if os.path.isdir(os.path.join(tile_dir, folder)):\n",
        "            for file in os.listdir(os.path.join(tile_dir, folder)):\n",
        "                if file.endswith(\".png\"):\n",
        "                    loc_list.append((os.path.join(os.path.join(tile_dir, folder), file), folder))\n",
        "    return loc_list\n",
        "\n",
        "\n",
        "\n",
        "IMAGE_SIZE = (224, 224)\n",
        "\n",
        "# Loop over all subframes, loading each into a list\n",
        "tf_data_train, tf_data_test, tf_data_val = [], [], []\n",
        "tf_dataset_train_v2, tf_dataset_test_v2, tf_dataset_val_v2 = [], [], []\n",
        "\n",
        "# Update the batch and buffer size as per your model requirements\n",
        "buffer_size = 64\n",
        "batch_size = 5\n",
        "\n",
        "for subdir, split in zip(subdirs, subdir_splits):\n",
        "    full_path = data_head_dir / subdir\n",
        "    if split=='validate':\n",
        "        tf_data_val.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
        "    elif split=='train':\n",
        "        tf_data_train.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
        "    elif split=='test':\n",
        "        tf_data_test.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
        "\n",
        "random.shuffle(tf_data_train)\n",
        "img_list, label_list = zip(*tf_data_train)\n",
        "img_list_t = tf.convert_to_tensor(img_list)\n",
        "lb_list_t = tf.convert_to_tensor(label_list)\n",
        "\n",
        "tf_dataset_train_v2= tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
        "tf_dataset_train_v2= tf_dataset_train_v2.map(load_and_preprocess_v2, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "tf_dataset_train_v2 = tf_dataset_train_v2.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
        "\n",
        "random.shuffle(tf_data_val)\n",
        "img_list, label_list = zip(*tf_data_val)\n",
        "img_list_t = tf.convert_to_tensor(img_list)\n",
        "lb_list_t = tf.convert_to_tensor(label_list)\n",
        "\n",
        "tf_dataset_val_v2 = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
        "tf_dataset_val_v2 = tf_dataset_val_v2.map(load_and_preprocess_v2, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "tf_dataset_val_v2 = tf_dataset_val_v2.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
        "\n",
        "random.shuffle(tf_data_test)\n",
        "img_list, label_list = zip(*tf_data_test)\n",
        "img_list_t = tf.convert_to_tensor(img_list)\n",
        "lb_list_t = tf.convert_to_tensor(label_list)\n",
        "\n",
        "tf_dataset_test_v2 = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
        "tf_dataset_test_v2 = tf_dataset_test_v2.map(load_and_preprocess_v2, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "tf_dataset_test_v2 = tf_dataset_test_v2.shuffle(buffer_size=buffer_size).batch(batch_size)"
      ],
      "metadata": {
        "id": "ac5a_M84Hd2k"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Dataset Length:\", len(tf_dataset_train_v2))\n",
        "print(\"Validation Dataset Length:\", len(tf_dataset_val_v2))\n",
        "print(\"Testing Dataset Length:\", len(tf_dataset_test_v2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pDi1UKqmP1Z",
        "outputId": "7202a8e4-d248-454b-c5de-e09dc66ccd64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Length: 442\n",
            "Validation Dataset Length: 247\n",
            "Testing Dataset Length: 250\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training CNN + MLP\n",
        "###### i. To perform empirical regularization, crop, randomly zoom, rotate, flip, con- trast, and translate images in your training set for image augmentation. You can use various tools to do this, including OpenCV."
      ],
      "metadata": {
        "id": "9i3IEw1itrlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7Ekkpaj0Z-N",
        "outputId": "97862bc5-eab4-417c-baaa-0c7de2a68fb2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "#import tensorflow_addons as tfa\n",
        "import math\n",
        "\n",
        "def augment(image, label):\n",
        "    # Randomly apply brightness and contrast adjustments\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "\n",
        "    # Randomly apply rotation\n",
        "    angle = tf.random.uniform([], minval=-math.pi/4, maxval=math.pi/4)\n",
        "    image = tfa.image.rotate(image, angle)\n",
        "\n",
        "    # Randomly flip left-right\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "    # Randomly flip up-down\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "\n",
        "    # Randomly adjust hue and saturation\n",
        "    image = tf.image.random_hue(image, max_delta=0.2)\n",
        "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
        "\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "LCIZJH9uDHFh"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## d) Transfer Learning"
      ],
      "metadata": {
        "id": "PMFPxk3L9FW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### In this project, you will use pre-trained models (EfficientNetB0, ResNet50, and VGG16). For these pre-trained networks, you will only train the last fully connected layer, and will freeze all layers before them (i.e. we do not change their parameters during training) and use the outputs of the penultimate layer in the original pre-trained model as the features extracted from each image."
      ],
      "metadata": {
        "id": "SlkvY0rr9JCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the augment function to each element in the dataset\n",
        "tf_dataset_train = tf_dataset_train_v2.map(augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "RgNDF-NGZtXu"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### VGG16"
      ],
      "metadata": {
        "id": "3EGerHLSI_co"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import VGG16, efficientnet\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "\n",
        "# Create VGG16 base model with pre-trained weights (include_top=False to exclude dense layers)\n",
        "vgg_base = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "vgg_base.trainable = False  # Freeze\n",
        "\n",
        "model_vgg = Sequential([\n",
        "    vgg_base,\n",
        "    #Flatten(),\n",
        "    GlobalAveragePooling2D(),\n",
        "    BatchNormalization(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.3),  # 30% dropout rate\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model with ADAM optimizer and cross entropy loss\n",
        "model_vgg.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping with patience = 3 and model checkpointing\n",
        "early_stop_criteria = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_model_vgg.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "history_vgg = model_vgg.fit(\n",
        "    tf_dataset_train_v2,\n",
        "    epochs = 10,\n",
        "    validation_data=tf_dataset_val_v2,\n",
        "    callbacks=[early_stop_criteria, checkpoint]\n",
        ")"
      ],
      "metadata": {
        "id": "vmF6Wl8d9InV",
        "outputId": "8c2de087-3ddf-42a3-8051-e9266e27b189",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "  82/2824 [..............................] - ETA: 2:34:02 - loss: 0.3341 - accuracy: 0.8366"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Error rates\n",
        "vgg_train_error, vgg_validation_error = [], []\n",
        "\n",
        "for i in range(10):\n",
        "  train_err = 1 - history_vgg.history['accuracy'][i]\n",
        "  vgg_train_error.append(train_err)\n",
        "\n",
        "  val_err = 1 - history_vgg.history['val_accuracy'][i]\n",
        "  vgg_validation_error.append(val_err)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Training & Validation Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_vgg.history['loss'], label = 'Training Loss', marker = 'o')\n",
        "plt.plot(history_vgg.history['val_loss'], label = 'Validation Loss', marker = 'o')\n",
        "plt.title('VGG16 Model: Training & Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot for Training and Validation Error\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(vgg_train_error, label = 'Training Error', marker = 'o')\n",
        "plt.plot(vgg_validation_error, label = 'Validation Error', marker = 'o')\n",
        "plt.title('VGG16 Model: Training & Validation Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Error Rate')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ElG_2r9hKq3n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Report Precision, Recall, and F1 score for VGG model."
      ],
      "metadata": {
        "id": "sK7HIlRrXnm4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best model = least validation loss\n",
        "best_vgg_model = load_model('best_model.h5')\n",
        "\n",
        "predictions = best_vgg_model.predict(tf_dataset_test_v2)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# pull actual Labels from test data\n",
        "actual_labels = []\n",
        "for images, labels in tf_dataset_test_v2.unbatch():\n",
        "    actual_labels.append(labels.numpy())\n",
        "\n",
        "vgg_report = classification_report(np.array(actual_labels).flatten(), predicted_labels.flatten())\n",
        "print(\"VGG16 Model Classification Report:\")\n",
        "print(vgg_report)"
      ],
      "metadata": {
        "id": "f0LR4PzPXpda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet50"
      ],
      "metadata": {
        "id": "z9015yG5WJkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.applications.resnet import preprocess_input\n",
        "\n",
        "base_resnet = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_resnet.trainable = False  # Freeze\n",
        "\n",
        "model_resnet = Sequential([\n",
        "    base_resnet,\n",
        "    GlobalAveragePooling2D(),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model with ADAM optimizer and cross entropy loss\n",
        "model_resnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping with patience = 3 and model checkpointing\n",
        "early_stop_criteria = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_model_res.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "history_resnet = model_resnet.fit(\n",
        "    tf_dataset_train,\n",
        "    epochs=10,\n",
        "    validation_data=tf_dataset_val,\n",
        "    callbacks=[early_stop_criteria, checkpoint]\n",
        ")"
      ],
      "metadata": {
        "id": "Qs-3TU5SWO6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Error rates\n",
        "resnet_train_error, resnet_validation_error = [], []\n",
        "\n",
        "for i in range(10):\n",
        "  train_err = 1 - history_resnet.history['accuracy'][i]\n",
        "  resnet_train_error.append(train_err)\n",
        "\n",
        "  val_err = 1 - history_resnet.history['val_accuracy'][i]\n",
        "  resnet_validation_error.append(val_err)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Training & Validation Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_resnet.history['loss'], label = 'Training Loss', marker = 'o')\n",
        "plt.plot(history_resnet.history['val_loss'], label = 'Validation Loss', marker = 'o')\n",
        "plt.title('ResNet50 Model: Training & Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot for Training and Validation Error\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(resnet_train_error, label = 'Training Error', marker = 'o')\n",
        "plt.plot(resnet_validation_error, label = 'Validation Error', marker = 'o')\n",
        "plt.title('ResNet50 Model: Training & Validation Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Error Rate')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XfyqTDHPXDfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Report Precision, Recall, and F1 score for ResNet50 model."
      ],
      "metadata": {
        "id": "wqUF4LNfYESD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best model = least validation loss\n",
        "best_resnet_model = load_model('best_model.h5')\n",
        "\n",
        "# run model with test data\n",
        "resnet_predictions = best_resnet_model.predict(tf_dataset_test_v2)\n",
        "predicted_labels = np.argmax(resnet_predictions, axis=1)\n",
        "\n",
        "# pull actual Labels from test data\n",
        "actual_labels = []\n",
        "for images, labels in tf_dataset_test_v2.unbatch():\n",
        "    actual_labels.append(labels.numpy())\n",
        "\n",
        "resnet_report = classification_report(np.array(actual_labels).flatten(), predicted_labels.flatten())\n",
        "print(\"ResNet50 Model Classification Report:\")\n",
        "print(resnet_report)"
      ],
      "metadata": {
        "id": "h8K5jZSCYExP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EfficientNetB0"
      ],
      "metadata": {
        "id": "AZ1mg0AFZM5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input"
      ],
      "metadata": {
        "id": "pyuzinqTZQrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_effnet = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_effnet.trainable = False  # Freeze\n",
        "\n",
        "model_reffnet = Sequential([\n",
        "    base_effnet,\n",
        "    GlobalAveragePooling2D(),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model with ADAM optimizer and cross entropy loss\n",
        "model_reffnet.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping with patience = 3 and model checkpointing\n",
        "early_stop_criteria = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_model_res.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "history_effnet = model_reffnet.fit(\n",
        "    tf_dataset_train_v2,\n",
        "    epochs=10,\n",
        "    validation_data=tf_dataset_val_v2,\n",
        "    callbacks=[early_stop_criteria, checkpoint]\n",
        ")"
      ],
      "metadata": {
        "id": "GJXMw636ZXFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Error rates\n",
        "effnet_train_error, effnet_validation_error = [], []\n",
        "\n",
        "for i in range(10):\n",
        "  train_err = 1 - history_effnet.history['accuracy'][i]\n",
        "  effnet_train_error.append(train_err)\n",
        "\n",
        "  val_err = 1 - history_effnet.history['val_accuracy'][i]\n",
        "  effnet_validation_error.append(val_err)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Training & Validation Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_effnet.history['loss'], label = 'Training Loss', marker = 'o')\n",
        "plt.plot(history_effnet.history['val_loss'], label = 'Validation Loss', marker = 'o')\n",
        "plt.title('EfficientNetB0 Model: Training & Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot for Training and Validation Error\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(effnet_train_error, label = 'Training Error', marker = 'o')\n",
        "plt.plot(effnet_validation_error, label = 'Validation Error', marker = 'o')\n",
        "plt.title('EfficientNetB0 Model: Training & Validation Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Error Rate')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ri1d3ecgZ8DF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Report Precision, Recall, and F1 score for EfficientNetB0 model."
      ],
      "metadata": {
        "id": "7YPsq-MUaKoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best model = least validation loss\n",
        "best_effnet_model = load_model('best_model.h5')\n",
        "\n",
        "# run model with test data\n",
        "effnet_predictions = best_effnet_model.predict(tf_dataset_test_v2)\n",
        "predicted_labels = np.argmax(effnet_predictions, axis=1)\n",
        "\n",
        "# pull actual Labels from test data\n",
        "actual_labels = []\n",
        "for images, labels in tf_dataset_test_v2.unbatch():\n",
        "    actual_labels.append(labels.numpy())\n",
        "\n",
        "effnet_report = classification_report(np.array(actual_labels).flatten(), predicted_labels.flatten())\n",
        "print(\"EfficientNetB0 Model Classification Report:\")\n",
        "print(effnet_report)"
      ],
      "metadata": {
        "id": "vCszBZUsaK67"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}