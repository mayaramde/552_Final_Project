{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KF8J-OLcd1cg"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path as op\n",
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import shutil\n",
        "import logging\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from skimage import io\n",
        "import keras\n",
        "import math\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import random\n",
        "random_state = 42\n",
        "from PIL import Image\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow.keras import layers, models, regularizers, callbacks, preprocessing, utils, applications, optimizers\n",
        "from tensorflow.keras.models import Sequential, load_model\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, ReLU, MaxPooling2D, Dropout, Flatten, Dense, GlobalMaxPooling2D, Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img\n",
        "from tensorflow.keras.applications import efficientnet, vgg16, EfficientNetB0\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
        "\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install tensorflow"
      ],
      "metadata": {
        "id": "W44XINje6W30"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive to Colab notebook\n",
        "#drive.flush_and_unmount()\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF16z2MwgT85",
        "outputId": "3a1a4039-5c17-43a9-d5cf-89c818d7a3db"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploaded data.zip to my google drive. Now, unzip it and store in 552_project_data folder\n",
        "# zip_file_path = '/content/drive/MyDrive/Colab Notebooks/data.zip'\n",
        "# destination_folder = '/content/drive/MyDrive/552_project_data/'\n",
        "# !unzip \"$zip_file_path\" -d \"$destination_folder\""
      ],
      "metadata": {
        "id": "KQ6y6B2nh3ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QGkoMFtod1ch"
      },
      "outputs": [],
      "source": [
        "# Logging configuration\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                    datefmt='%H:%M:%S',\n",
        "                    format='%(asctime)s | %(levelname)-5s | %(module)-15s | %(message)s')\n",
        "\n",
        "#IMAGE_SIZE = (299, 299)  # All images contained in this dataset are 299x299 (originally, to match Inception v3 input size)\n",
        "SEED = 17\n",
        "\n",
        "# Head directory containing all image subframes. Update with the relative path of your data directory\n",
        "data_head_dir = Path('/content/drive/MyDrive/552_project_data/data')\n",
        "\n",
        "# Find all subframe directories\n",
        "subdirs = [Path(subdir.stem) for subdir in data_head_dir.iterdir() if subdir.is_dir()]\n",
        "src_image_ids = ['_'.join(a_path.name.split('_')[:3]) for a_path in subdirs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XPU_EQM8d1ci"
      },
      "outputs": [],
      "source": [
        "# Load train/val/test subframe IDs\n",
        "def load_text_ids(file_path):\n",
        "    \"\"\"Simple helper to load all lines from a text file\"\"\"\n",
        "    with open(file_path, 'r') as f:\n",
        "        lines = [line.strip() for line in f.readlines()]\n",
        "    return lines\n",
        "\n",
        "# Load the subframe names for the three data subsets\n",
        "train_ids = load_text_ids('/content/drive/MyDrive/Colab Notebooks/train_source_images.txt')\n",
        "validate_ids = load_text_ids('/content/drive/MyDrive/Colab Notebooks/val_source_images.txt')\n",
        "test_ids = load_text_ids('/content/drive/MyDrive/Colab Notebooks/test_source_images.txt')\n",
        "\n",
        "# Generate a list containing the dataset split for the matching subdirectory names\n",
        "subdir_splits = []\n",
        "for src_id in src_image_ids:\n",
        "    if src_id in train_ids:\n",
        "        subdir_splits.append('train')\n",
        "    elif src_id in validate_ids:\n",
        "        subdir_splits.append('validate')\n",
        "    elif(src_id in test_ids):\n",
        "        subdir_splits.append('test')\n",
        "    else:\n",
        "        logging.warning(f'{src_id}: Did not find designated split in train/validate/test list.')\n",
        "        subdir_splits.append(None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uoXB_Xcd1ci"
      },
      "source": [
        "### Loading and pre processing the data for (224, 244) image size\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Must resize the images and recreate train/test/validation data sets\n",
        "def load_and_preprocess_v2(img_loc, label):\n",
        "    def _inner_function_v2(img_loc, label):\n",
        "      img_loc_str = img_loc.numpy().decode('utf-8')\n",
        "      img = Image.open(img_loc_str).convert('RGB')\n",
        "      img = img.resize((224, 224), Image.ANTIALIAS) # resize\n",
        "      img = np.array(img)\n",
        "      img = img / 255.0\n",
        "      label = 1 if label.numpy().decode('utf-8') == 'frost' else 0\n",
        "      return img, label\n",
        "\n",
        "    # Wrap the Python function\n",
        "    X, y = tf.py_function(_inner_function_v2, [img_loc, label], [tf.float32, tf.int64])\n",
        "\n",
        "    # Set the shape of the tensors\n",
        "    X.set_shape([224, 224, 3])  # new image size\n",
        "    y.set_shape([])  # Scalar label\n",
        "    return X, y\n",
        "\n",
        "def load_subdir_data(dir_path, image_size, seed=None):\n",
        "    # Grab only the classes that (1) we want to keep and (2) exist in this directory\n",
        "    tile_dir = dir_path / Path('tiles')\n",
        "    label_dir = dir_path /Path('labels')\n",
        "\n",
        "    loc_list = []\n",
        "\n",
        "    for folder in os.listdir(tile_dir):\n",
        "        if os.path.isdir(os.path.join(tile_dir, folder)):\n",
        "            for file in os.listdir(os.path.join(tile_dir, folder)):\n",
        "                if file.endswith(\".png\"):\n",
        "                    loc_list.append((os.path.join(os.path.join(tile_dir, folder), file), folder))\n",
        "    return loc_list\n",
        "\n",
        "\n",
        "\n",
        "IMAGE_SIZE = (224, 224)\n",
        "\n",
        "# Loop over all subframes, loading each into a list\n",
        "tf_data_train, tf_data_test, tf_data_val = [], [], []\n",
        "tf_dataset_train_v2, tf_dataset_test_v2, tf_dataset_val_v2 = [], [], []\n",
        "\n",
        "# Update the batch and buffer size as per your model requirements\n",
        "buffer_size = 64\n",
        "batch_size = 5\n",
        "\n",
        "for subdir, split in zip(subdirs, subdir_splits):\n",
        "    full_path = data_head_dir / subdir\n",
        "    if split=='validate':\n",
        "        tf_data_val.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
        "    elif split=='train':\n",
        "        tf_data_train.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
        "    elif split=='test':\n",
        "        tf_data_test.extend(load_subdir_data(full_path, IMAGE_SIZE, SEED))\n",
        "\n",
        "random.shuffle(tf_data_train)\n",
        "img_list, label_list = zip(*tf_data_train)\n",
        "img_list_t = tf.convert_to_tensor(img_list)\n",
        "lb_list_t = tf.convert_to_tensor(label_list)\n",
        "\n",
        "tf_dataset_train_v2= tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
        "tf_dataset_train_v2= tf_dataset_train_v2.map(load_and_preprocess_v2, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "tf_dataset_train_v2 = tf_dataset_train_v2.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
        "\n",
        "random.shuffle(tf_data_val)\n",
        "img_list, label_list = zip(*tf_data_val)\n",
        "img_list_t = tf.convert_to_tensor(img_list)\n",
        "lb_list_t = tf.convert_to_tensor(label_list)\n",
        "\n",
        "tf_dataset_val_v2 = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
        "tf_dataset_val_v2 = tf_dataset_val_v2.map(load_and_preprocess_v2, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "tf_dataset_val_v2 = tf_dataset_val_v2.shuffle(buffer_size=buffer_size).batch(batch_size)\n",
        "\n",
        "random.shuffle(tf_data_test)\n",
        "img_list, label_list = zip(*tf_data_test)\n",
        "img_list_t = tf.convert_to_tensor(img_list)\n",
        "lb_list_t = tf.convert_to_tensor(label_list)\n",
        "\n",
        "tf_dataset_test_v2 = tf.data.Dataset.from_tensor_slices((img_list_t, lb_list_t))\n",
        "tf_dataset_test_v2 = tf_dataset_test_v2.map(load_and_preprocess_v2, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "tf_dataset_test_v2 = tf_dataset_test_v2.shuffle(buffer_size=buffer_size).batch(batch_size)"
      ],
      "metadata": {
        "id": "ac5a_M84Hd2k"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Dataset Length:\", len(tf_dataset_train_v2))\n",
        "print(\"Validation Dataset Length:\", len(tf_dataset_val_v2))\n",
        "print(\"Testing Dataset Length:\", len(tf_dataset_test_v2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pDi1UKqmP1Z",
        "outputId": "d5afa06a-6042-4ccf-f110-1f71fbc6b7aa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Dataset Length: 2824\n",
            "Validation Dataset Length: 1578\n",
            "Testing Dataset Length: 1595\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install tensorflow_addons\n",
        "import tensorflow_addons as tfa"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7Ekkpaj0Z-N",
        "outputId": "de7a4467-1e10-480e-b23c-3b7e3ca50b34"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow_addons\n",
            "  Downloading tensorflow_addons-0.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (611 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/611.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/611.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━\u001b[0m \u001b[32m573.4/611.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.8/611.8 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow_addons) (23.2)\n",
            "Collecting typeguard<3.0.0,>=2.7 (from tensorflow_addons)\n",
            "  Downloading typeguard-2.13.3-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: typeguard, tensorflow_addons\n",
            "Successfully installed tensorflow_addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment(image, label):\n",
        "    # apply brightness and contrast adjustments\n",
        "    image = tf.image.random_brightness(image, max_delta=0.1)\n",
        "    image = tf.image.random_contrast(image, lower=0.8, upper=1.2)\n",
        "\n",
        "    # apply rotation\n",
        "    angle = tf.random.uniform([], minval=-math.pi/4, maxval=math.pi/4)\n",
        "    image = tfa.image.rotate(image, angle)\n",
        "\n",
        "    # flip left-right\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "\n",
        "    # flip up-down\n",
        "    image = tf.image.random_flip_up_down(image)\n",
        "\n",
        "    # adjust hue and saturation\n",
        "    image = tf.image.random_hue(image, max_delta=0.2)\n",
        "    image = tf.image.random_saturation(image, lower=0.8, upper=1.2)\n",
        "\n",
        "    return image, label\n",
        "\n",
        "# Apply the augment function to each element in the dataset\n",
        "tf_dataset_train_v2 = tf_dataset_train_v2.map(augment, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
      ],
      "metadata": {
        "id": "LCIZJH9uDHFh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## d) Transfer Learning"
      ],
      "metadata": {
        "id": "PMFPxk3L9FW5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### In this project, you will use pre-trained models (EfficientNetB0, ResNet50, and VGG16). For these pre-trained networks, you will only train the last fully connected layer, and will freeze all layers before them (i.e. we do not change their parameters during training) and use the outputs of the penultimate layer in the original pre-trained model as the features extracted from each image."
      ],
      "metadata": {
        "id": "SlkvY0rr9JCD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet50"
      ],
      "metadata": {
        "id": "z9015yG5WJkF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "base_res = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "base_res.trainable = False  # Freeze\n",
        "\n",
        "model_res = Sequential([\n",
        "    base_res,\n",
        "    GlobalAveragePooling2D(),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.3),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(2, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model with ADAM optimizer and cross entropy loss\n",
        "model_res.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Early stopping with patience = 3 and model checkpointing\n",
        "early_stop_criteria = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('best_model_res.h5', monitor='val_loss', save_best_only=True)\n",
        "\n",
        "history_resnet = model_res.fit(\n",
        "    tf_dataset_train_v2,\n",
        "    epochs=10,\n",
        "    validation_data=tf_dataset_val_v2,\n",
        "    callbacks=[early_stop_criteria, checkpoint]\n",
        ")\n",
        "\n",
        "# Save model at each epoch\n",
        "for epoch in range(1, 11):\n",
        "    model_res.save(f'resnet_model_at_epoch_{epoch}.h5')"
      ],
      "metadata": {
        "id": "Qs-3TU5SWO6t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7fd2d4c-c016-44e5-90ee-aa0aaa7e71c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n",
            "Epoch 1/10\n",
            " 800/2824 [=======>......................] - ETA: 31:34 - loss: 0.4447 - accuracy: 0.7962"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plot the training and validation errors vs. epochs."
      ],
      "metadata": {
        "id": "RCfeUPTKqrQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Error rates\n",
        "resnet_train_error, resnet_validation_error = [], []\n",
        "\n",
        "train_accuracy = history_resnet.history['accuracy']\n",
        "val_accuracy = history_resnet.history['val_accuracy']\n",
        "\n",
        "for i in range(len(train_accuracy)):\n",
        "    resnet_train_error.append(1 - train_accuracy[i])\n",
        "    resnet_validation_error.append(1 - val_accuracy[i])\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "# Training & Validation Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_resnet.history['loss'], label = 'Training Loss', marker = 'o')\n",
        "plt.plot(history_resnet.history['val_loss'], label = 'Validation Loss', marker = 'o')\n",
        "plt.title('ResNet50 Model: Training & Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "# Plot for Training and Validation Error\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(resnet_train_error, label = 'Training Error', marker = 'o')\n",
        "plt.plot(resnet_validation_error, label = 'Validation Error', marker = 'o')\n",
        "plt.title('ResNet50 Model: Training & Validation Error')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Error Rate')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XfyqTDHPXDfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Report Precision, Recall, and F1 score for ResNet50 model."
      ],
      "metadata": {
        "id": "wqUF4LNfYESD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best model = least validation loss\n",
        "best_resnet_model = load_model('best_model_res.h5')\n",
        "\n",
        "# run model with test data\n",
        "resnet_predictions = best_resnet_model.predict(tf_dataset_test_v2)\n",
        "predicted_labels = np.argmax(resnet_predictions, axis=1)\n",
        "\n",
        "# pull actual Labels from test data\n",
        "actual_labels = []\n",
        "for images, labels in tf_dataset_test_v2.unbatch():\n",
        "    actual_labels.append(labels.numpy())\n",
        "\n",
        "resnet_report = classification_report(np.array(actual_labels).flatten(), predicted_labels.flatten())\n",
        "print(\"ResNet50 Model Classification Report:\")\n",
        "print(resnet_report)"
      ],
      "metadata": {
        "id": "h8K5jZSCYExP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5750746b-c410-4f5d-82dd-8154583c8d0a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1595/1595 [==============================] - 1625s 1s/step\n",
            "ResNet50 Model Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.01      0.02      2198\n",
            "           1       0.72      0.99      0.84      5774\n",
            "\n",
            "    accuracy                           0.72      7972\n",
            "   macro avg       0.50      0.50      0.43      7972\n",
            "weighted avg       0.60      0.72      0.61      7972\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}